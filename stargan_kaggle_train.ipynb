{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 37705,
          "sourceType": "datasetVersion",
          "datasetId": 29561
        },
        {
          "sourceId": 8118479,
          "sourceType": "datasetVersion",
          "datasetId": 4796403
        }
      ],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# path-ul catre dataset-ul cu cod\n",
        "sys.path.insert(1, '/kaggle/input/pycode')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T17:16:09.926583Z",
          "iopub.execute_input": "2024-04-14T17:16:09.927269Z",
          "iopub.status.idle": "2024-04-14T17:16:09.931516Z",
          "shell.execute_reply.started": "2024-04-14T17:16:09.927237Z",
          "shell.execute_reply": "2024-04-14T17:16:09.930435Z"
        },
        "trusted": true,
        "id": "x2pSeUdSvGP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fisierul generic_dataset.py modificat\n",
        "# import glob\n",
        "# import ntpath\n",
        "# import random\n",
        "# from PIL import Image\n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision import transforms as T\n",
        "\n",
        "\n",
        "# CELEBA_FORMAT_DATASET = 1\n",
        "# RAFD_FORMAT_DATASET = 2\n",
        "\n",
        "\n",
        "# class GenericDataset(Dataset):\n",
        "#     \"\"\"Dataset description class\"\"\"\n",
        "#     def __init__(self, path, format, selected_attrs, mode, **kwargs):\n",
        "#         self.path = path\n",
        "#         self.format = format\n",
        "#         self.selected_attrs = selected_attrs\n",
        "#         self.mode = mode\n",
        "#         self.transform = kwargs.get(\"transform\", get_default_transforms(mode))\n",
        "\n",
        "#         self.attr2idx = {}\n",
        "#         self.idx2attr = {}\n",
        "#         self.train_dataset = []\n",
        "#         self.test_dataset = []\n",
        "\n",
        "#         if self.format == CELEBA_FORMAT_DATASET:\n",
        "#             self.preprocess_celeba()\n",
        "#         else:\n",
        "#             self.preprocess_rafd()\n",
        "\n",
        "#     def preprocess_celeba(self):\n",
        "#         lines = [line.strip() for line in open(\"/kaggle/input/pycode/list_attr.txt\", \"r\")]\n",
        "#         cnt_samples, all_attr_names = int(lines[0]), lines[1].split()\n",
        "#         assert set(self.selected_attrs).issubset(set(all_attr_names)), \"Invalid selected attrs\"\n",
        "#         for i, attr_name in enumerate(all_attr_names):\n",
        "#             self.attr2idx[attr_name] = i\n",
        "#             self.idx2attr[i] = attr_name\n",
        "\n",
        "#         lines = lines[2:]\n",
        "#         random.seed(1234)\n",
        "#         random.shuffle(lines)\n",
        "#         for i, line in enumerate(lines):\n",
        "#             split = line.split()\n",
        "#             filename = split[0]\n",
        "#             values = split[1:]\n",
        "\n",
        "#             label = []\n",
        "#             for attr_name in self.selected_attrs:\n",
        "#                 idx = self.attr2idx[attr_name]\n",
        "#                 label.append(values[idx] == '1')\n",
        "\n",
        "#             if (i+1) <= max(1, cnt_samples // 50):\n",
        "#                 self.test_dataset.append([self.path + \"/img_align_celeba/img_align_celeba/\" + filename, label])\n",
        "#             else:\n",
        "#                 self.train_dataset.append([self.path + \"/img_align_celeba/img_align_celeba/\" + filename, label])\n",
        "\n",
        "#         print(f'Finished preprocessing CelebA-format dataset at {self.path}: {len(self.train_dataset)} train images, {len(self.test_dataset)} test images')\n",
        "\n",
        "#     def preprocess_rafd(self):\n",
        "#         categs = set(ntpath.basename(categ_folder) for categ_folder in glob.glob(self.path + \"/train/*\"))\n",
        "#         categs_test = set(ntpath.basename(categ_folder) for categ_folder in glob.glob(self.path + \"/test/*\"))\n",
        "#         assert categs == categs_test, \"Train and test do not have the same categories\"\n",
        "#         assert set(self.selected_attrs).issubset(categs), \"Invalid selected attrs\"\n",
        "#         for i, attr_name in enumerate(categs):\n",
        "#             self.attr2idx[attr_name] = i\n",
        "#             self.idx2attr[i] = attr_name\n",
        "\n",
        "#         for subfolder in [\"train\", \"test\"]:\n",
        "#             dataset = self.train_dataset if subfolder == \"train\" else self.test_dataset\n",
        "#             for categ in categs:\n",
        "#                 if categ not in self.selected_attrs:\n",
        "#                     continue\n",
        "#                 for img_path in glob.glob(self.path + f\"/{subfolder}/{categ}/*.jpg\"):\n",
        "#                     label = [(attr == categ) for attr in self.selected_attrs]\n",
        "#                     dataset.append([img_path, label])\n",
        "\n",
        "#         print(f'Finished preprocessing RAFD-format dataset at {self.path}: {len(self.train_dataset)} train images, {len(self.test_dataset)} test images')\n",
        "\n",
        "#     def dataset_format(self):\n",
        "#         return self.format\n",
        "\n",
        "#     def label_size(self):\n",
        "#         return len(self.selected_attrs)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         dataset = self.train_dataset if self.mode == \"train\" else self.test_dataset\n",
        "#         return len(dataset)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         dataset = self.train_dataset if self.mode == \"train\" else self.test_dataset\n",
        "#         filename, label = dataset[index]\n",
        "#         image = Image.open(filename)\n",
        "#         return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "\n",
        "# class VisualizationDataset(Dataset):\n",
        "#     def __init__(self, path):\n",
        "#         self.img_paths = []\n",
        "#         for img_path in glob.glob(f'{path}/*.jpg'):\n",
        "#             self.img_paths.append(img_path)\n",
        "\n",
        "#         self.transform = get_default_transforms(\"test\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_paths)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         filename = self.img_paths[index]\n",
        "#         image = Image.open(filename)\n",
        "#         return self.transform(image)\n",
        "\n",
        "\n",
        "# def get_default_transforms(mode):\n",
        "#     transform = []\n",
        "#     if mode == \"train\":\n",
        "#         transform.append(T.RandomHorizontalFlip())\n",
        "#     transform.append(T.CenterCrop(178))\n",
        "#     transform.append(T.Resize(128))\n",
        "#     transform.append(T.ToTensor())\n",
        "#     transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
        "#     transform = T.Compose(transform)\n",
        "#     return transform\n",
        "\n",
        "\n",
        "# def get_loader(dataset, batch_size, mode='train', num_workers=1):\n",
        "#     \"\"\"Build and return a data loader.\"\"\"\n",
        "#     data_loader = DataLoader(dataset=dataset,\n",
        "#                             batch_size=batch_size,\n",
        "#                             shuffle=(mode=='train'),\n",
        "#                             num_workers=num_workers)\n",
        "#     return data_loader\n"
      ],
      "metadata": {
        "id": "U-H1bEy2vZnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from generic_dataset import GenericDataset, VisualizationDataset, get_loader\n",
        "from generic_dataset import CELEBA_FORMAT_DATASET, RAFD_FORMAT_DATASET\n",
        "from custom_solver import CustomSolver"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T17:16:42.396276Z",
          "iopub.execute_input": "2024-04-14T17:16:42.397016Z",
          "iopub.status.idle": "2024-04-14T17:16:42.427519Z",
          "shell.execute_reply.started": "2024-04-14T17:16:42.396982Z",
          "shell.execute_reply": "2024-04-14T17:16:42.426544Z"
        },
        "trusted": true,
        "id": "MKzQjY89vGP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "project_root_dir = Path(\"/kaggle/input/celeba-dataset\")\n",
        "celeba_dir = project_root_dir\n",
        "visualization_dir = Path(\"/kaggle/working/vis/\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T17:17:55.426108Z",
          "iopub.execute_input": "2024-04-14T17:17:55.426505Z",
          "iopub.status.idle": "2024-04-14T17:17:55.432441Z",
          "shell.execute_reply.started": "2024-04-14T17:17:55.426471Z",
          "shell.execute_reply": "2024-04-14T17:17:55.431457Z"
        },
        "trusted": true,
        "id": "v0vgrPi7vGP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celeba dataset\n",
        "celeba_selected_attrs = [\"Male\", \"Young\", \"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\"]\n",
        "celeba = GenericDataset(str(celeba_dir), CELEBA_FORMAT_DATASET, celeba_selected_attrs, \"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0fK1VGDE48m",
        "outputId": "e848b1aa-2cab-4a1d-abca-78b7b98c1775",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:18:01.851548Z",
          "iopub.execute_input": "2024-04-14T17:18:01.851882Z",
          "iopub.status.idle": "2024-04-14T17:18:03.976171Z",
          "shell.execute_reply.started": "2024-04-14T17:18:01.851857Z",
          "shell.execute_reply": "2024-04-14T17:18:03.975254Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Finished preprocessing CelebA-format dataset at /kaggle/input/celeba-dataset: 198548 train images, 4051 test images\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "os.makedirs(\"/kaggle/working/vis\")\n",
        "cnt = 0\n",
        "for i in range(1, 200000, 1000):\n",
        "    cnt += 1\n",
        "    img_name = str(i).zfill(6)\n",
        "    src = f'/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/{img_name}.jpg'\n",
        "    dest = f'/kaggle/working/vis/{cnt+1}.jpg'\n",
        "    shutil.copyfile(src, dest)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-14T17:16:55.918902Z",
          "iopub.execute_input": "2024-04-14T17:16:55.919227Z",
          "iopub.status.idle": "2024-04-14T17:16:57.117625Z",
          "shell.execute_reply.started": "2024-04-14T17:16:55.919202Z",
          "shell.execute_reply": "2024-04-14T17:16:57.116827Z"
        },
        "trusted": true,
        "id": "kl3Ujl5avGP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization dataset\n",
        "vis_dataset = VisualizationDataset(visualization_dir)\n",
        "print(len(vis_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Ikqx-3FX-u",
        "outputId": "90d5be80-a32d-4f9a-b9d8-b3936b93024d",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:16:58.625822Z",
          "iopub.execute_input": "2024-04-14T17:16:58.626190Z",
          "iopub.status.idle": "2024-04-14T17:16:58.632972Z",
          "shell.execute_reply.started": "2024-04-14T17:16:58.626160Z",
          "shell.execute_reply": "2024-04-14T17:16:58.631956Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "200\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/kaggle/working/exps/\")"
      ],
      "metadata": {
        "id": "bJanJkZkFP8S",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:17:01.991388Z",
          "iopub.execute_input": "2024-04-14T17:17:01.992018Z",
          "iopub.status.idle": "2024-04-14T17:17:01.996553Z",
          "shell.execute_reply.started": "2024-04-14T17:17:01.991986Z",
          "shell.execute_reply": "2024-04-14T17:17:01.995457Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = {\n",
        "    \"num_iters\": 20000,\n",
        "    \"num_iters_decay\": 0,\n",
        "    \"batch_size\": 64,\n",
        "    \"lr_update_step\": 2,\n",
        "    \"n_critic\": 5,\n",
        "    \"log_step\": 500,\n",
        "    \"vis_step\": 5000,\n",
        "    \"model_save_step\": 5000,\n",
        "}"
      ],
      "metadata": {
        "id": "_yq6MAcC7jyI",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:18:56.224943Z",
          "iopub.execute_input": "2024-04-14T17:18:56.225338Z",
          "iopub.status.idle": "2024-04-14T17:18:56.230466Z",
          "shell.execute_reply.started": "2024-04-14T17:18:56.225310Z",
          "shell.execute_reply": "2024-04-14T17:18:56.229542Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 1000 iters only on Celeba\n",
        "solver = CustomSolver([celeba], vis_dataset, Path(\"/kaggle/working/exps/\"), **settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKOVZZCPEjoJ",
        "outputId": "e2ce18eb-f32e-4740-9bbb-3ca217e61d10",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:18:58.443651Z",
          "iopub.execute_input": "2024-04-14T17:18:58.444464Z",
          "iopub.status.idle": "2024-04-14T17:18:59.064326Z",
          "shell.execute_reply.started": "2024-04-14T17:18:58.444403Z",
          "shell.execute_reply": "2024-04-14T17:18:59.063342Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "sum_label_size: 5, mask_vector_size: 1\nZero padding left: [0]\nZero padding right: [0]\nWorkdir set at /kaggle/working/exps\nGenerator(\n  (main): Sequential(\n    (0): Conv2d(9, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): ResidualBlock(\n      (main): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (16): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (17): ReLU(inplace=True)\n    (18): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (19): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (20): ReLU(inplace=True)\n    (21): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n    (22): Tanh()\n  )\n)\nG\nThe number of parameters: 8433664\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (3): LeakyReLU(negative_slope=0.01)\n    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (7): LeakyReLU(negative_slope=0.01)\n    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (9): LeakyReLU(negative_slope=0.01)\n    (10): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (11): LeakyReLU(negative_slope=0.01)\n  )\n  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (conv2): Conv2d(2048, 5, kernel_size=(2, 2), stride=(1, 1), bias=False)\n)\nD\nThe number of parameters: 44762048\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 apr 2024 20:19\n",
        "solver.train_multi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "flHvGVaNEjld",
        "outputId": "da4a491f-d168-47ee-dec5-ad53fe0602e7",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:19:01.758122Z",
          "iopub.execute_input": "2024-04-14T17:19:01.758530Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Start training...\nElapsed [0:05:17], Iteration [500/20000], Dataset [1], D/loss_real: -11.0378, D/loss_fake: 5.7954, D/loss_cls: 2.5621, D/loss_gp: 0.0562, G/loss_fake: -5.2391, G/loss_rec: 0.2793, G/loss_cls: 3.5305\nElapsed [0:10:34], Iteration [1000/20000], Dataset [1], D/loss_real: -9.4586, D/loss_fake: 5.6576, D/loss_cls: 1.8490, D/loss_gp: 0.0258, G/loss_fake: -4.8365, G/loss_rec: 0.2612, G/loss_cls: 2.6164\nElapsed [0:15:51], Iteration [1500/20000], Dataset [1], D/loss_real: -7.0314, D/loss_fake: 3.2213, D/loss_cls: 1.4697, D/loss_gp: 0.0398, G/loss_fake: -2.8455, G/loss_rec: 0.2561, G/loss_cls: 1.5486\nElapsed [0:21:08], Iteration [2000/20000], Dataset [1], D/loss_real: -5.2789, D/loss_fake: 1.3641, D/loss_cls: 1.5574, D/loss_gp: 0.0424, G/loss_fake: -0.5763, G/loss_rec: 0.2051, G/loss_cls: 1.7158\nElapsed [0:26:25], Iteration [2500/20000], Dataset [1], D/loss_real: -5.9204, D/loss_fake: 3.1334, D/loss_cls: 1.3650, D/loss_gp: 0.0136, G/loss_fake: -3.7567, G/loss_rec: 0.1980, G/loss_cls: 1.0571\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r file.zip /kaggle/working/exps/checkpoints/20000-G.ckpt\n"
      ],
      "metadata": {
        "id": "52gU98vNQ0Hf",
        "execution": {
          "iopub.status.busy": "2024-04-14T17:11:17.302220Z",
          "iopub.execute_input": "2024-04-14T17:11:17.302611Z",
          "iopub.status.idle": "2024-04-14T17:11:20.170905Z",
          "shell.execute_reply.started": "2024-04-14T17:11:17.302579Z",
          "shell.execute_reply": "2024-04-14T17:11:20.169775Z"
        },
        "trusted": true,
        "outputId": "2d470265-b876-4ba3-c191-b87312a4ef26"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "  adding: kaggle/working/exps/checkpoints/1000-G.ckpt (deflated 8%)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r file.zip /kaggle/working/exps/checkpoints/20000-D.ckpt"
      ],
      "metadata": {
        "id": "kbYiGeD9vGP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}